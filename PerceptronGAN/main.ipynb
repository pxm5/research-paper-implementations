{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mod = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features, out_features=128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mod(X)\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "lr = 3e-4\n",
    "z_dim = 64\n",
    "image_dim = 28*28*1\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "disc = Discriminator(image_dim)\n",
    "gene = Generator(z_dim, image_dim)\n",
    "fixed_noise = torch.randn((batch_size, z_dim))\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50]; LOSS_D: 0.6686; LOSS_G: 0.7528 \n",
      "Epoch [1/50]; LOSS_D: 0.4858; LOSS_G: 0.9529 \n",
      "Epoch [2/50]; LOSS_D: 0.6602; LOSS_G: 0.8707 \n",
      "Epoch [3/50]; LOSS_D: 0.9369; LOSS_G: 0.5660 \n",
      "Epoch [4/50]; LOSS_D: 0.4630; LOSS_G: 1.3248 \n",
      "Epoch [5/50]; LOSS_D: 0.3622; LOSS_G: 1.4009 \n",
      "Epoch [6/50]; LOSS_D: 0.3937; LOSS_G: 1.5424 \n",
      "Epoch [7/50]; LOSS_D: 0.8544; LOSS_G: 0.8167 \n",
      "Epoch [8/50]; LOSS_D: 0.3180; LOSS_G: 1.8629 \n",
      "Epoch [9/50]; LOSS_D: 0.5092; LOSS_G: 1.1593 \n",
      "Epoch [10/50]; LOSS_D: 0.6733; LOSS_G: 1.3894 \n",
      "Epoch [11/50]; LOSS_D: 0.5260; LOSS_G: 1.5212 \n",
      "Epoch [12/50]; LOSS_D: 0.6039; LOSS_G: 0.9596 \n",
      "Epoch [13/50]; LOSS_D: 0.6464; LOSS_G: 0.9831 \n",
      "Epoch [14/50]; LOSS_D: 0.5588; LOSS_G: 1.3560 \n",
      "Epoch [15/50]; LOSS_D: 0.7834; LOSS_G: 0.8340 \n",
      "Epoch [16/50]; LOSS_D: 0.5843; LOSS_G: 1.5093 \n",
      "Epoch [17/50]; LOSS_D: 0.7564; LOSS_G: 1.0407 \n",
      "Epoch [18/50]; LOSS_D: 0.5870; LOSS_G: 0.9592 \n",
      "Epoch [19/50]; LOSS_D: 0.4748; LOSS_G: 1.2606 \n",
      "Epoch [20/50]; LOSS_D: 0.6965; LOSS_G: 0.9699 \n",
      "Epoch [21/50]; LOSS_D: 0.6039; LOSS_G: 0.9382 \n",
      "Epoch [22/50]; LOSS_D: 0.6073; LOSS_G: 0.9998 \n",
      "Epoch [23/50]; LOSS_D: 0.6408; LOSS_G: 1.0503 \n",
      "Epoch [24/50]; LOSS_D: 0.6564; LOSS_G: 0.9260 \n",
      "Epoch [25/50]; LOSS_D: 0.5136; LOSS_G: 1.5844 \n",
      "Epoch [26/50]; LOSS_D: 0.6033; LOSS_G: 1.0690 \n",
      "Epoch [27/50]; LOSS_D: 0.6620; LOSS_G: 1.0174 \n",
      "Epoch [28/50]; LOSS_D: 0.5653; LOSS_G: 1.3038 \n",
      "Epoch [29/50]; LOSS_D: 0.5378; LOSS_G: 1.2920 \n",
      "Epoch [30/50]; LOSS_D: 0.6521; LOSS_G: 0.9740 \n",
      "Epoch [31/50]; LOSS_D: 0.7714; LOSS_G: 0.9212 \n",
      "Epoch [32/50]; LOSS_D: 0.6280; LOSS_G: 1.1888 \n",
      "Epoch [33/50]; LOSS_D: 0.4712; LOSS_G: 1.1957 \n",
      "Epoch [34/50]; LOSS_D: 0.6913; LOSS_G: 1.2352 \n",
      "Epoch [35/50]; LOSS_D: 0.5388; LOSS_G: 1.1090 \n",
      "Epoch [36/50]; LOSS_D: 0.5774; LOSS_G: 1.0118 \n",
      "Epoch [37/50]; LOSS_D: 0.6128; LOSS_G: 0.8531 \n",
      "Epoch [38/50]; LOSS_D: 0.6452; LOSS_G: 1.0899 \n",
      "Epoch [39/50]; LOSS_D: 0.5862; LOSS_G: 1.0593 \n",
      "Epoch [40/50]; LOSS_D: 0.5138; LOSS_G: 1.0787 \n",
      "Epoch [41/50]; LOSS_D: 0.5817; LOSS_G: 1.1339 \n",
      "Epoch [42/50]; LOSS_D: 0.6560; LOSS_G: 0.9077 \n",
      "Epoch [43/50]; LOSS_D: 0.5775; LOSS_G: 1.0604 \n",
      "Epoch [44/50]; LOSS_D: 0.7245; LOSS_G: 0.7796 \n",
      "Epoch [45/50]; LOSS_D: 0.6427; LOSS_G: 0.7277 \n",
      "Epoch [46/50]; LOSS_D: 0.5848; LOSS_G: 1.0076 \n",
      "Epoch [47/50]; LOSS_D: 0.5687; LOSS_G: 1.0250 \n",
      "Epoch [48/50]; LOSS_D: 0.5985; LOSS_G: 1.1681 \n",
      "Epoch [49/50]; LOSS_D: 0.6263; LOSS_G: 1.0138 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset = datasets.MNIST(root = \"dataset/\", transform=transforms, download=True)\n",
    "loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "optim_disc = optim.Adam(disc.parameters(), lr =lr)\n",
    "optim_gene = optim.Adam(gene.parameters(), lr =lr)\n",
    "critereon = nn.BCELoss()\n",
    "\n",
    "writer_fake = SummaryWriter(f'runs/GAN_MNIST/fake')\n",
    "writer_real = SummaryWriter(f'runs/GAN_MNIST/real')\n",
    "step = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.view(-1, 784)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Training Disc: \n",
    "        noise = torch.randn(batch_size, z_dim)\n",
    "        fake = gene(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = critereon(disc_real, torch.ones_like(disc_real))\n",
    "\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = critereon(disc_fake, torch.zeros_like(disc_fake))\n",
    "\n",
    "        lossD = (lossD_real + lossD_fake)/2 \n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph = True)\n",
    "        optim_disc.step()\n",
    "\n",
    "        ### Training Gene:\n",
    "\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = critereon(output, torch.ones_like(output))\n",
    "        gene.zero_grad()\n",
    "        lossG.backward()\n",
    "        optim_gene.step()\n",
    "\n",
    "\n",
    "        ### Code for TENSORBOARD\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{epochs}]; LOSS_D: {lossD:.4f}; LOSS_G: {lossG:.4f} \"\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                fake = gene(fixed_noise).reshape(-1, 1, 28, 28) # 28x28x1 is the picture dimensions\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step+=1\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
