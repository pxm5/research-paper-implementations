{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Try adding batch normalization later\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        self.conv1 = self._block(1, 256, 3, 2, 1, self.relu)\n",
    "        self.conv2 = self._block(256, 256, 3, 2, 1, self.relu)\n",
    "\n",
    "        # self.conv1 = nn.Conv2d(1, 256, kernel_size=3, stride =2, padding=1)\n",
    "        # self.bn1 = nn.BatchNorm2d(256)\n",
    "        # self.conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "        # self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.fc1 = nn.Linear(256 * 7 * 7, 1)\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding, activation):\n",
    "\n",
    "        return nn.Sequential(*[\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activation\n",
    "        ])\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = self.conv1(X)\n",
    "        out = self.conv2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.sigmoid(self.fc1(out))\n",
    "\n",
    "        return out\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(z_dim, 256 * 7 * 7)\n",
    "        self.bn1 = nn.BatchNorm1d(256*7*7)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.conv1 = self._block(256, 128, 4, 2, 1, self.relu)\n",
    "        self.conv2 = self._block(128, 64, 4, 2, 1, self.relu)\n",
    "        self.conv3 = self._block(64, 1, 3, 1, 1, self.tanh)\n",
    "\n",
    "\n",
    "        # self.conv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride = 2, padding=1)\n",
    "        # self.bn2 = nn.BatchNorm2d(128)\n",
    "        # self.conv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding =1)\n",
    "        # self.bn3 = nn.BatchNorm2d(64)\n",
    "        # self.conv3 = nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding =1)\n",
    "\n",
    "        \n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding, activation):\n",
    "        return nn.Sequential(*[\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activation,\n",
    "        ])\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.relu(self.bn1(self.fc1(X)))\n",
    "        out = out.view(out.size(0), 256, 7, 7)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper params\n",
    "lr = 3e-4\n",
    "z_dim = 64\n",
    "image_dim = 28 * 28 * 1\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "disc = Discriminator()\n",
    "gene = Generator(z_dim)\n",
    "fixed_noise = torch.randn((batch_size, z_dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50]; lossD: 0.0004; lossG: 13.6932\n",
      "Epoch [1/50]; lossD: 0.0000; lossG: 12.0279\n",
      "Epoch [2/50]; lossD: 0.0000; lossG: 13.7280\n",
      "Epoch [3/50]; lossD: 0.0000; lossG: 15.2406\n",
      "Epoch [4/50]; lossD: 0.0000; lossG: 16.1687\n",
      "Epoch [5/50]; lossD: 0.0214; lossG: 5.2874\n",
      "Epoch [6/50]; lossD: 0.0463; lossG: 2.6583\n",
      "Epoch [7/50]; lossD: 0.1735; lossG: 2.7121\n",
      "Epoch [8/50]; lossD: 0.2397; lossG: 2.4559\n",
      "Epoch [9/50]; lossD: 0.1702; lossG: 3.2550\n",
      "Epoch [10/50]; lossD: 0.1584; lossG: 2.2367\n",
      "Epoch [11/50]; lossD: 0.2307; lossG: 3.2700\n",
      "Epoch [12/50]; lossD: 0.2097; lossG: 1.9792\n",
      "Epoch [13/50]; lossD: 0.2402; lossG: 2.7962\n",
      "Epoch [14/50]; lossD: 0.2690; lossG: 1.9417\n",
      "Epoch [15/50]; lossD: 0.1579; lossG: 3.1928\n",
      "Epoch [16/50]; lossD: 0.2352; lossG: 2.3110\n",
      "Epoch [17/50]; lossD: 0.3720; lossG: 2.5588\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m lossD = (lossD_real + lossD_fake)/\u001b[32m2\u001b[39m\n\u001b[32m     27\u001b[39m disc.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mlossD\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m optim_disc.step()\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Generator\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\OneDrive\\Documents\\Generative\\generative\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\OneDrive\\Documents\\Generative\\generative\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\OneDrive\\Documents\\Generative\\generative\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST(root='dataset/', transform=transform, download=True)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optim_disc = torch.optim.Adam(params=disc.parameters(), lr=lr)\n",
    "optim_gene = torch.optim.Adam(params=gene.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "writer_fake = SummaryWriter('runs/fake')\n",
    "writer_real = SummaryWriter('runs/real')\n",
    "step = 1\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for idx, (real, _) in enumerate(loader):\n",
    "        batch_size = real.shape[0]\n",
    "        noise = torch.randn(batch_size, z_dim)\n",
    "        fake = gene(noise)\n",
    "\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "\n",
    "        lossD = (lossD_real + lossD_fake)/2\n",
    "\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        optim_disc.step()\n",
    "\n",
    "        # Generator\n",
    "        out = disc(fake).view(-1)\n",
    "        lossG = criterion(out, torch.ones_like(out))\n",
    "        gene.zero_grad()\n",
    "        lossG.backward()\n",
    "        optim_gene.step()\n",
    "\n",
    "        if idx == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}]; lossD: {lossD:.4f}; lossG: {lossG:.4f}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gene(fixed_noise).view(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(real, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step+=1\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
